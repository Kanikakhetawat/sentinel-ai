package com.phonepe.sentinelai.core.model;

import lombok.Data;

import java.util.Map;

/**
 *
 */
@Data
public class ModelUsageStats {

    @Data
    public static class PromptTokenDetails {
        /**
         * Cached tokens present in the prompt.
         */
        private int cachedTokens = 0;
        /**
         * Audio input tokens present in the prompt.
         */
        private int audioTokens = 0;
    }

    @Data
    public static class ResponseTokenDetails {
        /**
         * Tokens generated by the model for reasoning.
         */
        private int reasoningTokens = 0;
        /**
         * When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion.
         */
        private int acceptedPredictionTokens = 0;
        /**
         * When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion
         * . However, like reasoning tokens, these tokens are still counted in the total completion tokens for
         * purposes of billing, output, and context window limits.
         */
        private int rejectedPredictionTokens = 0;
        /**
         * Audio input tokens generated by the model.
         */
        private int audioTokens;
    }

    /**
     * Number of request made for this run
     */
    private int requestsForRun = 0;

    /**
     * Tool calls made for this run
     */
    private int toolCallsForRun = 0;

    /**
     * Number of request/prompt tokens used in this run. Eqv to "prompt_tokens" param in open-ai usage object.
     */
    private int requestTokens = 0;

    /**
     * Number of completion/response tokens used in this run. Eqv to "completion_tokens" param in open-ai usage object.
     */
    private int responseTokens = 0;

    /**
     * Total tokens used in the whole run. Eqv to "total_tokens" param in open-ai usage object.
     */
    private int totalTokens = 0;

    /**
     * Token usage for prompts
     */
    private PromptTokenDetails requestTokenDetails = new PromptTokenDetails();

    /**
     * Token usage for responses
     */
    private ResponseTokenDetails responseTokenDetails = new ResponseTokenDetails();

    // Total tokens used in the whole run, should generally be equal to `requestTokens + responseTokens`.
    private Map<String, Integer> details = null;
    // Any extra details returned by the model.


}
